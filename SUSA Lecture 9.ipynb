{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors** (kNN) is a simple, yet powerful algorithm that can be used to solve classification problems. \n",
    "\n",
    "The core idea is that inputs should be classified with other inputs that have similar features.\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "<img src='KVisual.png' width=\"300\" height=\"300\">\n",
    "<center> Image from: https://www.mdpi.com/2076-3417/8/1/28 </center>\n",
    "\n",
    "The procedure for classifying an input $X$ for any general problem is as follows:\n",
    "1. Pick a value of $k$. Note: You can choose any value for $k$. What works best varies from data set to data set, which can be determined by trial and error. \n",
    "2. Find the $k$ nearest neighbors to your input, according to your distance metric.\n",
    "3. Count the number of neighbors in each category.\n",
    "4. Categorize the input based on the majority. \n",
    "\n",
    "Note: kNN has $O(1)$ training time! This is because there is no training. (Think about why)\n",
    "\n",
    "### Checkpoint\n",
    "1. In the figure above, what would the $?$ be categorized as when $k = 3$?\n",
    "2. What about when $k = 11$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers Here**  \n",
    "1.\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "Once we've chosen a value for $k$, we can imagine the **decision boundaries** associated with our model. \n",
    "\n",
    "<img src='decisionBoundary.png' width=\"300\" height=\"300\">\n",
    "<center> Image from: \"The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman.  </center>\n",
    "\n",
    "From every point in our feature space, we can calculate what an input at that location would be classified as. \n",
    "\n",
    "The decision boundaries divide our feature space into sections that would be assigned the same classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance with kNN\n",
    "\n",
    "Now, lets take a look at how our models are affected by the value of $k$.\n",
    "\n",
    "### A Warm Up\n",
    "\n",
    "<img src='knn-variance.png' width=\"700\" height=\"700\">\n",
    "<center> Image from: http://ljdursi.github.io/ML-for-scientists/#1 </center>\n",
    "\n",
    "For some intuition take a moment to consider these extreme cases: \n",
    "> 1. How would our model behave if $k = n$?  \n",
    "> 2. What if $k = 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers Here**\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some questions to follow:\n",
    "> 3. Does $k=1$ or $k=n$ produce more jagged decision boundaries?\n",
    "> 4. Does this indicate higher or lower variancei within our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Analysis\n",
    "\n",
    "\n",
    "<img src='knn_k.png' width=\"900\" height=\"900\">\n",
    "<center> Image from: Classification of Hand-written Digits (3) by DeWilde </center>\n",
    "\n",
    "**k = 1**  \n",
    "When $k=1$, our model always picks the closest neighbor and classifies the input respectively. In this case our decision boundaries would be jagged from overfitting to every single detail, and small changes in the training dataset would cause the decision boundary to shift a lot. Since the model will be very different depending on the training data, $k=1$ would create models with **high variance** and **low bias**. \n",
    "\n",
    "**k = n**  \n",
    "On the other hand, when $k = n$, every single input would be categorized as the same. Consider the classification problem in the image above. When $k=n$ we'll look "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (old)\n",
    "The **k-Nearest Neighbors** (kNN) algorithm is one of the simplest models. The core idea is that a similar set of features should have the same label. For example if we receive an image $A$ as input where we would like to classify the digit, we could look at what other images look like $A$ in our training set. If we were doing $5$-nearest neighbors, we would find the $5$ images closest to $A$ in our data set, and return the most common digit among the $5$. In general, you may choose any value for $k$, $5$ may not be the best choice. Note, this has a $O(1)$ training time! This is the fastest algorithm for training, as there is no training!\n",
    "\n",
    "However, some questions immediately arise. How do you determine how close two images are? Why choose $5$, not $10$ or $100$? There are other consequences as well; you need to look through your entire dataset each time to determine the $k$ closest images, which could take a long time if your training set is huge. The prediction time for kNN is $O(n)$, which is much slower than something like linear regression, where the prediction is $O(1)$.\n",
    "\n",
    "We will address these questions and the shortcomings of kNN.\n",
    "\n",
    "A few conceptual questions for understanding:\n",
    "1. In binary classification (two classes), why is choosing an odd value for $k$ better than an even value?\n",
    "2. Given two separate ordered pairs of two values, $(a,b)$ and $(x,y)$, what possibilities are there for calculating the \"distance\" between them? What are the differences between approaches?\n",
    "3. Assume we are doing image classification. List any possible issues with image classification.\n",
    "4. What does 1-NN mean? If we have $n$ training data, what is $n$-NN? What are some of the *tradeoffs* for varying $k$ between $1$ and $n$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# These lines set up the plotting functionality and formatting.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "#You may need to pip install pandas/matplotlib\n",
    "\n",
    "#Given a movie title, this returns the frequency of given words\n",
    "def wordFreq(movieTitle,words):\n",
    "    #Change movieTitle to lower case\n",
    "    movieTitle=movieTitle.lower()\n",
    "    \n",
    "    #Change words to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    #Check if movie title is found\n",
    "    try:\n",
    "        movie = movies[movies[\"Title\"]==movieTitle]\n",
    "    except:\n",
    "        print(\"Movie title not found!\")\n",
    "        return \"\"\n",
    "    \n",
    "    #Check if given words are not found\n",
    "    try:\n",
    "        wordFrequencies = movie[words].as_matrix()[0]\n",
    "    except:\n",
    "        print(\"Words not found\")\n",
    "        return \"\"\n",
    "    \n",
    "    return wordFrequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "      <th># Votes</th>\n",
       "      <th># Words</th>\n",
       "      <th>i</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>foster</th>\n",
       "      <th>pub</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>garrison</th>\n",
       "      <th>grammoo</th>\n",
       "      <th>chimney</th>\n",
       "      <th>bikini</th>\n",
       "      <th>richter</th>\n",
       "      <th>psychopath</th>\n",
       "      <th>fling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the terminator</td>\n",
       "      <td>action</td>\n",
       "      <td>1984</td>\n",
       "      <td>8.1</td>\n",
       "      <td>183538</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batman</td>\n",
       "      <td>action</td>\n",
       "      <td>1989</td>\n",
       "      <td>7.6</td>\n",
       "      <td>112731</td>\n",
       "      <td>2836</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.033850</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.028209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tomorrow never dies</td>\n",
       "      <td>action</td>\n",
       "      <td>1997</td>\n",
       "      <td>6.4</td>\n",
       "      <td>47198</td>\n",
       "      <td>4215</td>\n",
       "      <td>0.028707</td>\n",
       "      <td>0.054330</td>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batman forever</td>\n",
       "      <td>action</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.4</td>\n",
       "      <td>77223</td>\n",
       "      <td>3032</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.042216</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>supergirl</td>\n",
       "      <td>action</td>\n",
       "      <td>1984</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6576</td>\n",
       "      <td>3842</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>0.026288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title   Genre  Year  Rating  # Votes  # Words         i  \\\n",
       "0       the terminator  action  1984     8.1   183538     1849  0.040022   \n",
       "1               batman  action  1989     7.6   112731     2836  0.051481   \n",
       "2  tomorrow never dies  action  1997     6.4    47198     4215  0.028707   \n",
       "3       batman forever  action  1995     5.4    77223     3032  0.036609   \n",
       "4            supergirl  action  1984     4.1     6576     3842  0.041905   \n",
       "\n",
       "        the        to         a  ...    foster  pub  vegetarian  garrison  \\\n",
       "0  0.043807  0.025419  0.024878  ...       0.0  0.0         0.0       0.0   \n",
       "1  0.033850  0.023977  0.028209  ...       0.0  0.0         0.0       0.0   \n",
       "2  0.054330  0.030368  0.021827  ...       0.0  0.0         0.0       0.0   \n",
       "3  0.042216  0.020449  0.031003  ...       0.0  0.0         0.0       0.0   \n",
       "4  0.032275  0.028891  0.026288  ...       0.0  0.0         0.0       0.0   \n",
       "\n",
       "   grammoo  chimney  bikini  richter  psychopath  fling  \n",
       "0      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "1      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "2      0.0      0.0     0.0      0.0    0.000237    0.0  \n",
       "3      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "4      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "\n",
       "[5 rows x 5006 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see what our dataset looks like!\n",
    "movies = pd.read_csv('movies.csv')\n",
    "movies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "      <th># Votes</th>\n",
       "      <th># Words</th>\n",
       "      <th>i</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>foster</th>\n",
       "      <th>pub</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>garrison</th>\n",
       "      <th>grammoo</th>\n",
       "      <th>chimney</th>\n",
       "      <th>bikini</th>\n",
       "      <th>richter</th>\n",
       "      <th>psychopath</th>\n",
       "      <th>fling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the terminator</td>\n",
       "      <td>action</td>\n",
       "      <td>1984</td>\n",
       "      <td>8.1</td>\n",
       "      <td>183538</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batman</td>\n",
       "      <td>action</td>\n",
       "      <td>1989</td>\n",
       "      <td>7.6</td>\n",
       "      <td>112731</td>\n",
       "      <td>2836</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.033850</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.028209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tomorrow never dies</td>\n",
       "      <td>action</td>\n",
       "      <td>1997</td>\n",
       "      <td>6.4</td>\n",
       "      <td>47198</td>\n",
       "      <td>4215</td>\n",
       "      <td>0.028707</td>\n",
       "      <td>0.054330</td>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batman forever</td>\n",
       "      <td>action</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.4</td>\n",
       "      <td>77223</td>\n",
       "      <td>3032</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.042216</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>supergirl</td>\n",
       "      <td>action</td>\n",
       "      <td>1984</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6576</td>\n",
       "      <td>3842</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>0.026288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title   Genre  Year  Rating  # Votes  # Words         i  \\\n",
       "0       the terminator  action  1984     8.1   183538     1849  0.040022   \n",
       "1               batman  action  1989     7.6   112731     2836  0.051481   \n",
       "2  tomorrow never dies  action  1997     6.4    47198     4215  0.028707   \n",
       "3       batman forever  action  1995     5.4    77223     3032  0.036609   \n",
       "4            supergirl  action  1984     4.1     6576     3842  0.041905   \n",
       "\n",
       "        the        to         a  ...    foster  pub  vegetarian  garrison  \\\n",
       "0  0.043807  0.025419  0.024878  ...       0.0  0.0         0.0       0.0   \n",
       "1  0.033850  0.023977  0.028209  ...       0.0  0.0         0.0       0.0   \n",
       "2  0.054330  0.030368  0.021827  ...       0.0  0.0         0.0       0.0   \n",
       "3  0.042216  0.020449  0.031003  ...       0.0  0.0         0.0       0.0   \n",
       "4  0.032275  0.028891  0.026288  ...       0.0  0.0         0.0       0.0   \n",
       "\n",
       "   grammoo  chimney  bikini  richter  psychopath  fling  \n",
       "0      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "1      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "2      0.0      0.0     0.0      0.0    0.000237    0.0  \n",
       "3      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "4      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "\n",
       "[5 rows x 5006 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see what our dataset looks like!\n",
    "movies = pd.read_csv('movies.csv')\n",
    "movies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[0.00035261 0.05148096]\n"
     ]
    }
   ],
   "source": [
    "#What type is movies?\n",
    "print(type(movies))\n",
    "\n",
    "#What is the frequency of the words \"hey\" and \"i\" in the matrix? Try some yourself! \n",
    "print(wordFreq(\"batman\",[\"Hey\",\"i\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classification and Feature Selection\n",
    "Our goal is to be able to classify songs based on the frequency of various words in the script. However it is not feasible to use all the words as that is very calculation intensive. An alternative is to select certain features, but what features do we select? One method to look at which words are often in romance movies but not action, and vice versa. This is called **feature selection**. \n",
    "\n",
    "First, we will separate the data into training and validation data. Next, we may create some elementary functions such as the distance between movies, getting movies as pandas series, and finding the $k$ movies closest to some given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 80 training and 20 validation\n",
    "trainingPercentage = 80/100\n",
    "numMovies = movies.shape[0]\n",
    "numTraining = (int)(numMovies*trainingPercentage)\n",
    "numValidation = numMovies - numTraining\n",
    "\n",
    "#Training Set\n",
    "trainingSet = movies[0:numTraining]\n",
    "\n",
    "#Validation Set\n",
    "validationSet = movies[numTraining:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate into action and romance\n",
    "action = trainingSet[trainingSet[\"Genre\"]==\"action\"]\n",
    "romance = trainingSet[trainingSet[\"Genre\"]==\"romance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given two movie titles mov1,mov2, and a list of words\n",
    "#distance returns the euclidean distance between the two movies using the words as features\n",
    "def distance(mov1,mov2,words):\n",
    "    mov1Freq=wordFreq(mov1,words)\n",
    "    mov2Freq=wordFreq(mov2,words)\n",
    "    return np.sqrt(sum((mov1Freq-mov2Freq)**2))\n",
    "\n",
    "#Given a movie title, this returns the row as a pandas series\n",
    "def getMovie(title):\n",
    "    title = title.lower()\n",
    "    return movies[movies[\"Title\"]==title].squeeze()\n",
    "\n",
    "#Given a movie as a panda series, determines the k closest movies using words as features\n",
    "#Returns the dataframe of movies\n",
    "def kShortestDistance(k,movie,movieSet,words):\n",
    "    distances=[]\n",
    "    #Iterate over all movies\n",
    "    for i in range(movieSet.shape[0]):\n",
    "        currMovieTitle = movieSet.iloc[i][\"Title\"]\n",
    "        #Get the distance of two movies from two movies \n",
    "        dist = distance(currMovieTitle,movie[\"Title\"],words)\n",
    "        distances.append((dist,i))\n",
    "    #Sort the array\n",
    "    distances = sorted(distances,key=lambda x:x[0])\n",
    "    #Get the indices of the movies\n",
    "    indices = [x[1] for x in distances]\n",
    "    return movieSet.iloc[indices[1:k+1]]\n",
    "\n",
    "#Faster kShortestDistance using subsetting\n",
    "def kShortestDistanceFast(k,movie,movieSet,words):\n",
    "    #Subset out the words\n",
    "    movieSubset = movieSet[words]\n",
    "    currMovie = movie[words].squeeze()\n",
    "    #Calculate Distances and sort\n",
    "    distances = ((movieSubset-currMovie)**2).sum(axis=1)\n",
    "    distances = distances.sort_values()\n",
    "    #Shift by the minimum index if the movies do not start at 0\n",
    "    indices = distances.index.tolist()\n",
    "    minIndex = min(indices)\n",
    "    shiftedIndices=(np.array(indices)-minIndex).tolist()\n",
    "    return movieSet.iloc[shiftedIndices[1:k+1]]\n",
    "\n",
    "#Given a list of movies, returns the majority genre\n",
    "def getMajority(nearestMovies):\n",
    "    numMovies = nearestMovies.shape[0]\n",
    "    #Count frequency of genres\n",
    "    counts = nearestMovies['Genre'].value_counts()\n",
    "    if len(counts)==1:\n",
    "        return [x[0] for x in counts.items()][0]\n",
    "    if counts[\"action\"]>numMovies/2:\n",
    "        return \"action\"\n",
    "    return \"romance\"\n",
    "\n",
    "#Given a dataset, a set of word features, and the value of k\n",
    "#Returns the percentage correct (0-100)\n",
    "def accuracy(dataset,features,k):\n",
    "    numCorrect = 0 \n",
    "    #Iterate over all movies\n",
    "    for i in range(dataset.shape[0]):\n",
    "        currMovie = dataset.iloc[i].squeeze()\n",
    "        currMovieGenre = currMovie[\"Genre\"]\n",
    "        #Calculate k closest movies\n",
    "        kClosest = kShortestDistanceFast(k,currMovie,dataset,features)\n",
    "        predGenre = getMajority(kClosest)\n",
    "        #Keep track of number of correct predictions\n",
    "        if predGenre == currMovieGenre:\n",
    "            numCorrect +=1\n",
    "    #Return accuracy as percentage\n",
    "    return numCorrect*1.0/dataset.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses \"power\" and \"love\" as features to find the $5$ closest movies to \"batman returns\". Then we get the majority of the genres of those five movies, and we find that batman returns is predicted to be action based on those $5$ movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'action'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use \"money\" and \"feel\" as features\n",
    "features = [\"power\",\"love\"]\n",
    "movie = getMovie(\"batman returns\")\n",
    "#Get the five closest movies to the \"batman returns\" using the training set\n",
    "closest=kShortestDistance(5,movie,trainingSet,features)\n",
    "#Given the closest movies, returns the majority \n",
    "#Represents the kNN Prediction\n",
    "getMajority(closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this word plot (courtesy of Data 8) to construct some of your own features!\n",
    "<img src='wordplot.png' width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.17616580310881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try with some of your own features!\n",
    "features = [\"power\",\"feel\"]\n",
    "k=5\n",
    "accuracy(trainingSet,features,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our chosen features\n",
    "features = [\"men\",\"power\",\"marri\",\"nice\",\"home\",\"captain\",\"move\",\"run\",\"world\",\"huh\",\"happi\",\"move\",\"write\",\"hello\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our own chosen features, we then use the training set to determine the optimal value for $k$. Afterwards, we use this value of $k$ to find the accuracy on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Determine the best value for k\n",
    "trainAccuracies = []\n",
    "numKValues = 30\n",
    "for i in range(numKValues):\n",
    "    acc =accuracy(trainingSet,features,2*i+1)\n",
    "    trainAccuracies.append(acc)\n",
    "xAxis = ([2*i+1 for i in range(numKValues)])\n",
    "plt.plot(xAxis,trainAccuracies)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous information about overfitting and underfitting, explain the shape of the graph! Why is the accuracy low for $k=1$ and as $k$ increases past $15$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 11\n",
      "Test Accuracy: 71.42857142857143\n"
     ]
    }
   ],
   "source": [
    "#Determine best value for k\n",
    "optimalK=xAxis[np.argmax(trainAccuracies)]\n",
    "\n",
    "#Best kNN was found with k=11\n",
    "print(\"Best k:\",optimalK)\n",
    "\n",
    "#Determine validation error with this value for k\n",
    "optimalKNNVal = accuracy(validationSet,features,optimalK)\n",
    "print(\"Test Accuracy:\",optimalKNNVal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the accuracy for the validation set lower than the training accuracy (about $75\\%$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance in $k$-Nearest Neighbors\n",
    "\n",
    "Let's investigate bias and variance in terms of kNN. A great dataset is the iris dataset, which contains three classes of irises and $50$ examples of each. The data set contains sepal and petal length and width. For the sake of clarity, I add some noise to the iris data so that the decision boundaries are not as clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9239862d68fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.02\u001b[0m  \u001b[0;31m# step size in the mesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "np.random.seed(189)\n",
    "\n",
    "#From http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "X = X + 0.3*np.random.randn(X.shape[0], X.shape[1])\n",
    "y[:] = iris.target\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "\n",
    "n_neighbors = 1\n",
    "weights ='uniform'\n",
    "\n",
    "\n",
    "def kNNForIris(num_neighbors,X,y,mweights = \"uniform\"):\n",
    "    plt.figure(figsize = (7,7))\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(num_neighbors, weights=weights)\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (num_neighbors, weights))\n",
    "    plt.show()\n",
    "    print(\"Training Accuracy: %.3f\"%(clf.score(X,y)))\n",
    "\n",
    "def determineError(X,Y,yTitle=\"Training Accuracy\"):\n",
    "    plt.figure(figsize = (7,7))\n",
    "\n",
    "        \n",
    "    errors = []\n",
    "    for num_neighbors in range(1,trainX.shape[0]):\n",
    "        clf = neighbors.KNeighborsClassifier(num_neighbors, weights=weights)\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        clf.fit(X,Y)\n",
    "\n",
    "        error = clf.score(X,Y)\n",
    "        errors.append(error)\n",
    "    \n",
    "    plt.plot(errors)\n",
    "    plt.ylabel(yTitle)\n",
    "    plt.xlabel('k Value')\n",
    "    plt.show()\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Different Values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kNNForIris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a8c9c3f73679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkNNForIris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkNNForIris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkNNForIris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kNNForIris' is not defined"
     ]
    }
   ],
   "source": [
    "kNNForIris(1,X,y)\n",
    "kNNForIris(6,X,y)\n",
    "kNNForIris(50,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundaries in $k$-Nearest Neighbors\n",
    "\n",
    "The plots above have red, green, and blue colored sections corresponding to **decision boundaries**. Decision boundaries are the boundaries in which a specific classication decision is chosen. For example, the red section represents the area where a new point would be classified as red, and similarly for blue and green. The decision boundary is the dividing line between the red and green sections. When comparing $k=1$ and $k=50$, there are major differences in the decision boundary. For $k=1$, the decision boundary is not a smooth and contains many small \"islands of classification\". The decision boundary is very complex and attempts to fit specifically to each data point. This is not necessary the best choice. If we consider $k=50$, then the decision boundary is far mor simple. The decision boundary is roughly composed of straight lines, for example the green region is roughly triangular.\n",
    "\n",
    "How do we bring bias and variance into these plots?\n",
    "\n",
    "## Bias in kNN\n",
    "The bias for kNN can be thought of classification accuracy. For small values of $k$, our model performs exceptionally on the training data. $k=1$ even does perfect classification! For large values of $k$, our models performs quite poorly, hardly better than random guessing. Thus bias increases as $k$ increases.\n",
    "\n",
    "Below is a plot of the training accuracy using the the training data to create decision boundaries and predict. The bias increases (accuracy is the opposite of error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'determineError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f6297b7ecf95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermineError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'determineError' is not defined"
     ]
    }
   ],
   "source": [
    "errors = determineError(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance in kNN\n",
    "Variance for kNN is best understood through the decision boundaries. With the smaller values of $k$, the decision boundaries are jagged and complex, but simple and straight for larger values of $k$. Thus variance decreases as $k$ increases. \n",
    "\n",
    "Below is a gif of the decision boundaries evolving as $k$ increases.\n",
    "<img src=\"knn.gif\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Tradeoff\n",
    "\n",
    "We have seen that increasing $k$ increases the bias but decreases the variance. But we want both low bias and variance! How do we balance bias and variance? This is known as the bias variance tradeoff. \n",
    "\n",
    "![alt text](BiasVarianceTradeoff.png \"Bias Variance Tradeoff\")\n",
    "\n",
    "Image from http://scott.fortmann-roe.com/docs/BiasVariance.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal model is one that minimizes the sum of the $\\text{bias}^2$ and variance. This is usually determined through testing various values and selecting the best one. A key notion of the trade off is that a central value is the best choice. When generating a model, it may seem that it is best to always minimize bias, because on average our models will do well. This is incorrect as in practice we only have one model (more on this later in the notebook!) and we would like consistent and good results with this single model. Although on average we may have low error, if we only have one opportunity it is very important to sacrifice some error to have greater confidence in our model.\n",
    "\n",
    "**Test for understanding**:\n",
    "Where does underfitting and overfitting lie in the graph above? How do they relate to bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "One solution is **ensemble learning**! Ensemble learning relies on the notion of aggregating multiple models and using the average classification of those models. We could create multiple decision trees for iris classification and average the prediction overall of the trees and use that as our prediction. What happens when we use a lot of trees? We get **random forest**. However, if we just created a bunch of decision trees, we would get the same decision tree each time. What we can do is sample what datapoints we would like our decision tree to train on, and select which subset of features we would like our decision tree to utilize. \n",
    "\n",
    "<img src=\"RandomForest.png\" width=\"60%\">\n",
    "\n",
    "Image from https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
