{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "**k-Nearest Neighbors** (kNN) is a simple, yet powerful algorithm that can be used to solve classification problems. \n",
    "\n",
    "The core idea is that inputs should be classified with other inputs that have similar features.\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "<img src='KVisual.png' width=\"300\" height=\"300\">\n",
    "<center> Image from: https://www.mdpi.com/2076-3417/8/1/28 </center>\n",
    "\n",
    "The procedure for classifying an input $X$ for any general problem is as follows:\n",
    "1. Pick a value of $k$. Note: You can choose any value for $k$. What works best varies from data set to data set, which can be determined by trial and error. \n",
    "2. Find the $k$ nearest neighbors to your input, according to your distance metric.\n",
    "3. Count the number of neighbors in each category.\n",
    "4. Categorize the input based on the majority. \n",
    "\n",
    "Note: kNN has $O(1)$ training time! This is because there is no training. (Think about why)\n",
    "\n",
    "### Checkpoint\n",
    "1. In the figure above, what would the $?$ be categorized as when $k = 3$?\n",
    "2. What about when $k = 11$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers Here**   \n",
    "1.  \n",
    "2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Time :) \n",
    "\n",
    "Now that we've gone over the bare bones of kNN, you guys get to implement it yourselves!\n",
    "\n",
    "Before we jump into coding we'll introduce you to the data we'll be looking at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "<img src='iris.jpg' width=\"250\" height=\"250\">\n",
    "<center> Image from: A Complete Guide to K-Nearest-Neighbors by Zakka </center>\n",
    "\n",
    "The dataset we'll be using is the [Iris Flower Dataset](https://archive.ics.uci.edu/ml/datasets/Iris). It contains a series of observations on three species of Iris (Iris setosa, Iris virginica, and Iris verisolor). Each observation contains four features: the *petal length, petal width, sepal length, and sepal width*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "np.random.seed(189)\n",
    "\n",
    "#importing the data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#y contains the correct classifications (0, 1, 2 for each type of Iris)\n",
    "y = iris.target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset (plength, pwidth, slength, swidth)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Iris Dataset (plength, pwidth, slength, swidth)\")\n",
    "print(iris.data[:9, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code it up\n",
    "The following helper functions will be useful!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the Euclidian distance between two iris data points.\n",
    "def distance(x1, x2):\n",
    "    #print(\"finding dist btwn:\", x1, x2)\n",
    "    for i in range(len(x1)):\n",
    "        sq_diff = np.square(x1-x2)\n",
    "    return np.sum(sq_diff)\n",
    "\n",
    "#Returns a numpy array containing the classifications of the k closest data points\n",
    "#k is the number of neighbors, iris is the input we are trying to classify\n",
    "#irisData is the training dataset, and y contains the classifications for the training dataset\n",
    "def getKClosestClassifications(k, iris, irisData, y):\n",
    "    distances = []\n",
    "    \n",
    "    #Get the distance from iris to every single classified iris\n",
    "    for i in range(irisData.shape[0]):\n",
    "        ##print(\"i\", i)\n",
    "        dist = distance(irisData[i], iris);\n",
    "        ##print(\"dist\", dist)\n",
    "        distances.append((dist,i))\n",
    "        \n",
    "    #Sort the array of distances\n",
    "    distances = sorted(distances,key = lambda x:x[0])\n",
    "    ##print(distances)\n",
    "    \n",
    "    #Get the indicies of the k closest irisis\n",
    "    indicies = [x[1] for x in distances[:k]]\n",
    "    \n",
    "    return [y[i] for i in indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it's your turn!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a function (irisKNN) that takes in parameters: \n",
    "     - k \n",
    "    - iris (the input to be classified)\n",
    "     - data (the training dataset)\n",
    "     - y (the classifications for the training dataset)\n",
    "\n",
    "     \n",
    "    And returns 0, 1, or 2 as the kNN classification of iris\n",
    "     \n",
    "    *Note the following functions may be useful\n",
    "        - getKClosestClassifications(k, iris, irisData, y)\n",
    "            => returns a list containing the classifications [0, 1, or 2] \n",
    "                for each of the k closest data points to iris\n",
    "        - lst.count(x)\n",
    "            => returns the count of value 'x' in list 'lst'\n",
    "'''\n",
    "\n",
    "def irisKNN(k, iris, data, y):\n",
    "    #YOUR CODE HERE #TO REMOVE\n",
    "    k_class = getKClosestClassifications(k, iris, data, y)\n",
    "    return max(k_class, key=k_class.count)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0 kNN: 0\n",
      "Expected: 1 kNN: 1\n",
      "Expected: 2 kNN: 2\n"
     ]
    }
   ],
   "source": [
    "sol0 = irisKNN(100, iris.data[3], iris.data, y)\n",
    "sol1 = irisKNN(3, iris.data[65], iris.data, y)\n",
    "sol2 = irisKNN(9, iris.data[125], iris.data, y)\n",
    "\n",
    "print(\"Expected: \" + str(y[3]) + \" kNN: \"+ str(sol0))\n",
    "print(\"Expected: \" + str(y[65]) + \" kNN: \"+ str(sol1))\n",
    "print(\"Expected: \" + str(y[125]) + \" kNN: \"+ str(sol2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congrats! You've now implemented kNN.**\n",
    "\n",
    "But you may have some questions. _What happens when the dataset is really really huge? What if the Euclidian distance isn't the best way to classify how close two points are? And last but not least, how do you chose a value for k?_\n",
    "\n",
    "We'll start by briefly answering the first two.   \n",
    "\n",
    "The prediction time for kNN is $O(n)$, which is much slower than something like linear regression, where teh prediction is $O(1)$. This is just one of the drawbacks of kNN which is known to be really computationally expensive.  \n",
    "\n",
    "The distance function can actually be customized and designed to suit your particular dataset. For example, say you're classifying something based on features f1 and f2, where f1 is much more important than f2. In that case you can adjust accordingly by weighting the distance between the two f1 values much higher than distance between the two f2 values.  \n",
    "\n",
    "As for the value of $k$, $k$ is something we pick depending on the model we are trying to build. You can kind of think of choosing k as turning a knob and seeing which produces the best results (trial and error). We'll delve deeper into the affect $k$ has on the bias and variance of the model in a bit.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "If we've chosen a value for $k$, we can imagine the **decision boundaries** associated with our model. \n",
    "\n",
    "The picture below shows the boundary between the blue and yellow points.\n",
    "\n",
    "<img src='decisionBoundary.png' width=\"300\" height=\"300\">\n",
    "<center> Image from: \"The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman.  </center>\n",
    "\n",
    "The decision boundaries divide our feature space into sections that would be assigned the same classification. So any inputs that fall within the yellow space or blue space would be classified as yellow and blue respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance with kNN\n",
    "\n",
    "Now, lets take a look at how our models are affected by the value of $k$.\n",
    "\n",
    "### A Warm Up\n",
    "\n",
    "<img src='knn-variance.png' width=\"700\" height=\"700\">\n",
    "<center> Image from: http://ljdursi.github.io/ML-for-scientists/#1 </center>\n",
    "\n",
    "For some intuition take a moment to consider these extreme cases: \n",
    "> 1. How would our model behave if $k = n$?  \n",
    "> 2. What if $k = 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers Here**\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some questions to follow:\n",
    "> 3. Does $k=1$ or $k=n$ produce more jagged decision boundaries?\n",
    "> 4. Does this indicate higher or lower variancei within our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Thoughts Here**\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Analysis\n",
    "\n",
    "\n",
    "<img src='knn_k.png' width=\"900\" height=\"900\">\n",
    "<center> Image from: Classification of Hand-written Digits (3) by DeWilde </center>\n",
    "\n",
    "**k = 1**  \n",
    "When $k=1$, our model always picks the closest neighbor and classifies the input respectively. In this case our decision boundaries would be jagged from overfitting to every single detail, and small changes in the training dataset would cause the decision boundary to shift a lot. Since the model will be very different depending on the training data, $k=1$ would create models with **high variance** and **low bias**. \n",
    "\n",
    "**k = n**  \n",
    "On the other hand, when $k = n$, every single input would be categorized as the same. Consider the classification problem in the image above. There are more yellow than purple points, so when $k=n$ we'll always categorize the input as yellow no matter where it's located in our feature space. Because of this, kNN with $k=n$ would result in models that contain **low variance** and **high bias**.\n",
    "\n",
    "#### tldr;\n",
    " - variance **decreases** as $k$ **increases**\n",
    " - bias **increases** as $k$ **increases**\n",
    " \n",
    " \n",
    "Below is a gif of the decision boundaries evolving as $k$ increases.\n",
    "<img src=\"knn.gif\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### Test Yourself  \n",
    "Which picture corresponds to high/low values of k?  \n",
    "\n",
    "![alt text](BiasVariance.jpg \"Bias Variance Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blanks  \n",
    " ______ increases as k increases  \n",
    " ______ decreases as k increases  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
