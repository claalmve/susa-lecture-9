{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification vs Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, over the past few lectures, we've been talking about different Machine Learning models that you can create to help you **fit to an possible underlying trend in a dataset** and **generalize well to other data points**. However, a lot of the different models we have been creating are what we call **regression models**. Today, we'll be talking about another type of modeling: **classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give a brief recap on what regression models are and what they do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input a set of continuous variables as features\n",
    "- Output a continuous value as its prediction\n",
    "- Not good with discrete variables or categorial variables\n",
    "- Example: predicting height based off of people's weights.\n",
    "- Regression models can range from Linear Regression models to Neural Networks (we'll be going into what those are next lecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from the Linear Regression lecture, regression models can be incredibly useful in terms of predicting different **continuous variables**, but not well with predicting **discrete variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in order to create models that are good at predicting discrete variables, we must introduce a new type of modelling: classification models. These models are useful for when we want to classify some sample inputs into classes or categories. They can:\n",
    "- Input lists of features or qualities of a sample and output a class or label\n",
    "- Can take in discrete or categorical variables\n",
    "- Can involve generating a probability for an output to be each different class, and selecting the class with the highest probability\n",
    "\n",
    "Today, we'll be focusing a lot of different classification models, and different circumstances under which these kinds of models would be useful, as well as the different implementations for these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Inuition on Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the following picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='machine_learning_classification_graph.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Let's say we introduced a new point $X$ into the picture above. What would you guess the type (or class) the point would be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the picture, the intuition behind predicting $X$ would be to just look at the different points nearby it! In other words, we can **predict the class of a point by looking at its closest neighbors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One real world example of where predicting the class fo a point based off of its neighbors is with an upcoming exam. Let's say you ask two different people who already took the exam how hard it was, so you can gauge how hard it might be for yourself. But let's also say between the two people you ask, one is super duper smart and the other is Pass/No-Passing the class. Their perspectives of how difficult the test can be incredibly different from another. The super duper smart person might say the test was pretty easy, which may have been a result of their intellect and the time they put into studying for the exam. The person taking the class Pass/No Pass might think the test was incredibly hard, which may have been a result of them putting in less effort into the class due to the fact that they only need to Pass it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that both people can have incredibly different responses (or classes) of how difficult the exam was, and both of their responses might not be useful to you, as their skill levels and the amount of preparation they put into the class may be very different from your owns. In order to accurately gauge how difficult the test might be, you want to ask people around you (in terms of skill level and amount of preparation)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's say you ask someone with around the same skill level/amount of preparation with you. That person could be a good indicator of how difficult the test might be for you, but asking that one person might not be enough. Let's say you wanted to ask more people in taking the same class what they thought of the exam. You could average out their responses, and get a general gist of how difficult the exam might be for yourself. However, if you continue to keep asking people, you might get to the point where you ask a number close to the size of the class! At that point, how difficult you think the exam might be for you is just how difficult the exam itself was on average. So, we can see that not only are **the types of people you ask** an important to factor consider, but also **the number of people** you ask can affect your result as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy that we used to determine how difficult the exam was going to be for yourself is implemented exactly how a famous algorithm in machine learning is implemented, called **k-Nearest Neighbors** (shortened as kNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've taken a good look at KNN and how useful it can be, let's take a look at a different type of modelling that also helps us classify points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees rely on the idea of **splitting up your data points with linear decision boundaries into different sections that are able to be classified by one class.** In order words, it's asking a series of yes or no questions to make our decision/estimation on which class an input point falls under."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a couple examples of what a decision trees look like:\n",
    "\n",
    "<img src='XKCD.png' width=35%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the tree above, we are able to answer a yes or no question at every step, and depending on our answer, we either went one way or another through the three. They are very synonymous to flowcharts, but we'll go into more intricacies with decision trees later on. In practice, with decision tree models, they won't have cycles, but they'll have the same logic structures. Here's another example of a decision tree made by Rosa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='meme.png' width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meme credit to $\\text{Rosa Choe}^{\\text{TM}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen a couple examples of decision trees, we can see how they can be used to help us classify a given input data point with a series of question to determine which class the data point lies within.\n",
    "\n",
    "**Question:** How might you go about making the different components of a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind making a good decision tree is optimizing our questions (or different steps in the decision tree) to be able to split up the data into as many different, pure classes as much as possible. Now the question that can arise is, what is a good method through which we can determine these different bounds to split upon? To answer that question, we introduce the idea of entropy, where we minimize the entropy, or randomness in each split section of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's first define what entropy is. In the context of machine learning, entropy is **the measure of disorder within different data sets.** But how can we go about measuring the disorder within each data set? With the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical equation for Entropy:\n",
    "\n",
    "$$H(\\textbf{p}) = -\\sum_i p_i * log_{2}(p_i)$$\n",
    "\n",
    "where $H(\\textbf{p})$ is equal to the total entropy of the data set, and $p_i$ is equal to the probability of something occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what does this mean in the context of decision trees? Well, let's take a look at a random scatter of different points that are classified blue and orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng0 = np.random.RandomState(0)\n",
    "rng1 = np.random.RandomState(1)\n",
    "x1 = rng0.randn(50)\n",
    "y1 = rng0.randn(50)\n",
    "x2 = rng1.randn(100)\n",
    "y2 = rng1.randn(100) - 3\n",
    "plt.scatter(x1, y1);\n",
    "plt.scatter(x2, y2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that a good decision boundary to split our data might be at the line `y = -1`. But in order to back our visual findings with numerical proof, let's calculate the entropy of the set of data we have (which is basically just the entire data set as of now), and the entropy of each split data set weighted by the proportion of data points in each section. Now that might seem like a lot, but we'll break it down into different steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we've provided the following helper function to calculate entropy given a list of probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities):\n",
    "    \"\"\"\n",
    "    Takes in probabilities as an array or list, and calculates the \n",
    "    Shannon's entropy of the set of data (defined above)\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for prob in probabilities:\n",
    "        total += prob * np.log2(prob)\n",
    "    return -total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's take a look at the entropy calculated from entire dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([100/150, 50/150]) # 2/3 of the points are orange, 1/3 of the points are blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it's about $0.92$! But what does this metric mean? Well, to see its significance, let's take a look at the different entropies of the two data subsets we get after we split the data with the line `y = -1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of the data with a horizontal line drawn at `y = -1`!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x1, y1);\n",
    "plt.scatter(x2, y2);\n",
    "plt.axhline(y=-1, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the entropies of each of the two data subsets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2466846702674598"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of orange and blue data points below the line y = 8.\n",
    "orange_count_below_neg1, blue_count_below_neg1 = 0, 0\n",
    "for y in y1:\n",
    "    if y < -1:\n",
    "        orange_count_below_neg1 += 1\n",
    "for y in y2:\n",
    "    if y < -1:\n",
    "        blue_count_below_neg1 += 1\n",
    "\n",
    "total_below_count = orange_count_below_neg1 + blue_count_below_neg1\n",
    "\n",
    "# Calculating the entropy of the subset with data points with y-values less than 8.\n",
    "bottom_entropy = entropy([orange_count_below_neg1 / total_below_count, blue_count_below_neg1 / total_below_count])\n",
    "\n",
    "# Calculating the entropy of the subset with respective to its proportion size\n",
    "bottom_entropy *= (total_below_count / 150) # 150 total data points\n",
    "bottom_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just from calculating the entropy in the subset of data points with y-values less than 1, and multiplying it by the proportion of data points in the lower subset, we see that it's already significantly lower than the entropy calculated from the entire dataset itself! However, we still need to calculate the entropy for the upper subset of data as well, and add those entropies up. Only if the summed entropies is less than the initial entropy level will we be able to conclude that this boundary we've created is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIY! Calculate the entropy of the subset of data points with y-values > -1 multiplied by the proportion of data points in that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Copy over what we did to calculate the bottom entropy, and change some things to get a value for the top entropy!\n",
    "orange_count_above_neg1, blue_count_above_neg1 = ..., ...\n",
    "\n",
    "top_entropy = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10666385349512605"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO REMOVE\n",
    "\n",
    "# Counting the number of orange and blue data points below the line y = 8.\n",
    "orange_count_above_neg1, blue_count_above_neg1 = 0, 0\n",
    "for y in y1:\n",
    "    if y > -1:\n",
    "        orange_count_above_neg1 += 1\n",
    "for y in y2:\n",
    "    if y > -1:\n",
    "        blue_count_above_neg1 += 1\n",
    "\n",
    "total_above_count = orange_count_above_neg1 + blue_count_above_neg1\n",
    "\n",
    "# Calculating the entropy of the subset with data points with y-values less than 8.\n",
    "top_entropy = entropy([orange_count_above_neg1 / total_above_count, blue_count_above_neg1 / total_above_count])\n",
    "\n",
    "# Calculating the entropy of the subset with respective to its proportion size\n",
    "top_entropy *= (total_above_count / 150) # 150 total data points\n",
    "top_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert top_entropy == 0.10666385349512605"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the total entropy of both subsets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35334852376258585"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_entropy = bottom_entropy + top_entropy\n",
    "total_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! From this process, we can clearly see that $0.35$ is less than $0.92$, so the decision boundary we introduced made the overall entropy of the data points lower! We've now got two subsets of the data that have lower entropy, meaning there is less disorder in terms of proportion between the two classes within each subset. We've officially made our first branch our decision tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great visualization for different entropies is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Entropy.png' width='50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say $Pr(X = 1)$ is the probability that you flips a heads, where heads is represented by $1$ and tails is represented by $0$. From this, we can see that the y-value, $H(X)$ (or calculated entropy), is at a minimum when the chance of flipping a heads is $0$ or $1$, but is at a maximum when the chance of flipping a heads is $0.5$. In other words, the data subset is the most random when there is an equal probability of all classes, and minimized when there are probabilites of classes that are equal to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Something to note:** When you calculate the entropy on a subset with data points that are all the same class, you run into a mathematical error, which is because $\\log_{2}(0)$ cannot be calculated. So, as an alternative to calculating $\\log{2}(0)$, we can bring in the following limit instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lim _{p\\to 0+}p\\log(p)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're able to see from a numerical approach on which decision boundaries would be effective in terms of classifying our data, but we still haven't established how we can up with the line `y = -1`! Clearly from the above example, we derived the line `y = -1` from looking at the data, but sometimes it's not so clear to determine where decision boundaries should lie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: **Is there a good strategy to determine which decision boundaries to calculate the total entropy of the subsets for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possibly Change the Following Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! There doesn't seem to an effective way of determining which decision boundaries to test, except through iterating all the possible decision boundaries and choosing the one that results in the lowest total calculated entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how entropy can help us determine which decision boundaries are good for our decision tree, let's get to constructing a decision tree of our own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct our own decision tree with the iris dataset by filling in the blanks below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier('entropy')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at some of the different parameters that were printed! We have things ranging from `min_samples_leaf` to `max_features`; all different parameters that can construct our desired decision tree differently. Now, like any machine learning model, let's FIT it against some training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4de5a9eeeadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Hint: Take a look a the documentation to see how we can fit our model against our iris.data and our iris.target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "model.fit(...) # Hint: Take a look a the documentation to see how we can fit our model against our iris.data and our iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we've constructed a decision tree off of the Iris data set, let's see what it looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8e07eb2c94441ca109fc8df2aa77c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='entropy', description='crit'), Text(value='best', description='split'), Dropâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helpers import plot_tree\n",
    "plot_tree('entropy', 'best', datasets.load_iris())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be able to tell that you can play around with the drop-down boxes and select which depth of tree you'd like to see, as well as input different kinds of datasets into the third argument of the `plot_tree` function. Try plugging in different kinds of data sets into the third argument of the function and see the different decision trees that pop up as a result! (Some data sets you can try out are `datasets.load_breast_cancer()` and `datasets.load_wine()`, but feel free to try other data sets that require classification!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we want to highlight is that these decision trees stopped making decision boundaries (or branches) when the subsets were completely pure (entropy = 0)! That means that all the data points within that subset were all of the same class, and that we were able to classify every data point perfectly. Now, we can begin to classify future data points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: **Is there anything wrong with creating our decision tree until all leaves are pure?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we might notice with this strategy of splitting data points until they reach \"pure\" data subsets is that the decision trees will be liable to **overfitting**! Let's take a look at a great example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DecisionTreeError.png\" width=\"100%\">\n",
    "Image from http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote17.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: **How do we go about fixing this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution to this problem of overfitting is limiting the number of splits your decision tree takes! By setting a certain quota for the number of splits your decision tree takes, we can ensure we don't end up with scenarios with the graph on the left, where we are too precisely creating decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another good example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plotPairwiseDecisionTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html\n",
    "# Max Depth 2\n",
    "plotPairwiseDecisionTrees(2)\n",
    "# Max Depth 4\n",
    "plotPairwiseDecisionTrees(4)\n",
    "# No Math Depth\n",
    "plotPairwiseDecisionTrees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the deeper our tree goes, the higher the variance is within the tree, as the decision tree is super tailored towards our training data, and could be totally different had we just added/removed a couple data points. However, it also has low bias, as it won't consistently classify certain data points incorrectly (it's too precise!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'd love to have a decision tree that had both low bias and low variance, but it seems like it's a tradeoff for one or the other. So, it'd be ideal to get the best of both worlds, and get low bias and low variance. But how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: **What if we got more models trained on our training set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of training more models on our training set introduces the idea of **ensemble learning**, which we will go into further in the next section, and help us solve our dilemna of wanting both low bias and low variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
